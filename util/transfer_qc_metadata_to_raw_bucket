#!/usr/bin/env python3

# Transfer locally saved QC'd metadata to the raw Google bucket
# Usage: 
#   Single dataset:
#     python3 transfer_qc_metadata_to_raw_bucket -t jakobsson -ds pmdbs-bulk-rnaseq -rv v4.0.1 -p
#
# Batch processing (datasets.csv field 1 is team, field 2 is dataset, field 3 is release version, no header):
#     while IFS=, read -r team dataset; do
#         python3 transfer_qc_metadata_to_raw_bucket -t "$team" -ds "$dataset" -p
#     done < datasets.csv
#
# Defaults to dry run unless -p flag is added!
# 
# NOTE on assumptions made:
# 1.   You have cloned asap-crn-cloud-dataset-metadata and its root is at the
# ---- same level as wf-common
# 2.   The metadata/ directory of the target dataset exists locally and contains
# ---- the finalized QC'd metadata structure (original/, cde/, and 
# ---- release/{release_version} dirs)
# 3.   The file_metadata/ directory exists locally and is populated with the files
# ---- artifacts.csv, raw_files.csv, and curated_files.csv

import argparse
import logging
from pathlib import Path
from common import gsync, strip_team_name, gcopy
from bucket_validation_utils import (
    check_bucket_exists,
    check_original_metadata_exists_locally,
    validate_local_metadata_structure
)


logging.basicConfig(
	level=logging.INFO,
	format="%(asctime)s - %(levelname)s - %(message)s"
)

repo_root = Path(__file__).resolve().parents[1]
metadata_root = repo_root.parent / "asap-crn-cloud-dataset-metadata"

# Selected files in file_metadata/ generated during QC to be synced to raw bucket
FILE_METADATA_FILES_TO_SYNC = [
    "artifacts.csv",
    "raw_files.csv",
    "curated_files.csv",
]


def main(args):

    dry_run = not args.promote
    
    team_name = strip_team_name(args.team_name)
    dataset_name = args.dataset_name
    dataset_name_long = f"{team_name}-{dataset_name}"
    release_version = args.release_version
    
    dataset_dir = metadata_root / "datasets" / dataset_name_long
    metadata_dir = dataset_dir / "metadata"
    file_metadata_dir = dataset_dir / "file_metadata"
    doi_dir = dataset_dir / "DOI"
    
    # No "team-" prefix for cohorts
    if team_name == "cohort":
        bucket_name = f"gs://asap-raw-{team_name}-{dataset_name}"
    else:
        bucket_name = f"gs://asap-raw-team-{team_name}-{dataset_name}"
    
    metadata_bucket = f"{bucket_name}/metadata"
    file_metadata_bucket = f"{bucket_name}/file_metadata"
    doi_bucket = f"{bucket_name}/DOI"
    
    check_bucket_exists(bucket_name)
    
    # Validate local metadata structure before transfer
    is_cohort = (team_name == "cohort")
    validate_local_metadata_structure(metadata_dir, is_cohort=is_cohort, release_version=release_version)

    # Transfers entire metadata/ directory
    if metadata_dir.exists():
        logging.info(f"Transferring local metadata directory to [{metadata_bucket}]")
        
        # Early exit if original metadata is missing: implies incomplete QC
        if not check_original_metadata_exists_locally(metadata_dir):
            raise ValueError(f"original/ directory is missing in local metadata dir: {metadata_dir}")
        
        gsync(source_path=str(metadata_dir), 
              destination_path=metadata_bucket, 
              dry_run=dry_run)
    else:
        raise ValueError(f"Local metadata directory not found: {metadata_dir}")

    # Transfer file_metadata/ files
    if file_metadata_dir.exists():
        logging.info(f"Transferring selected local file_metadata/ files to [{file_metadata_bucket}]")
        for file_name in FILE_METADATA_FILES_TO_SYNC:
            file_path = file_metadata_dir / file_name
            if file_path.exists():
                if dry_run:
                    logging.info(f"Would copy {file_path} to {file_metadata_bucket}/{file_name}")
                else:
                    logging.info(f"Transferring file: {file_name}")
                    gcopy(source_path=str(file_path), 
                          destination_path=f"{file_metadata_bucket}/{file_name}",
                          recursive=False)
            else:
                logging.warning(f"File not found, skipping: {file_name}")
    else:
        raise ValueError(f"Local file metadata directory not found: {file_metadata_dir}")
    
    # Transfers DOI/
    if doi_dir.exists():
        logging.info(f"Transferring local DOI/ directory to [{doi_bucket}]")
        
        gsync(source_path=str(doi_dir), 
              destination_path=doi_bucket, 
              dry_run=dry_run)
    else:
        logging.warning(f"Local DOI directory not found, skipping transfer: {doi_dir}")
    
    

if __name__ == "__main__":
    
    parser = argparse.ArgumentParser(
		description="Sync local metadata directories to raw bucket"
	)
    
    parser.add_argument(
        "-t",
        "--team_name",
        required=True,
        help="The team name of the dataset"
    )
    parser.add_argument(
        "-ds",
        "--dataset_name",
        required=True,
        help="The name of the dataset"
    )
    parser.add_argument(
        "-rv",
        "--release_version",
        required=True,
        help="The release version of the dataset (e.g., v4.0.1)"
    )
    parser.add_argument(
        "-p",
		"--promote",
		action="store_true",
		required=False,
		help="Promote data (default is dry run)."
	)

    args = parser.parse_args()
    main(args)
