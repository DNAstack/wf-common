#!/usr/bin/env python3

import argparse
import sys
import logging
import subprocess
from datetime import datetime, timezone
from google.cloud import storage
from common import (
	unembargoed_dev_buckets_and_workflow_version_outputs,
	list_teams,
	list_dirs,
	list_gs_files,
	read_manifest_files,
	md5_check,
	non_empty_check,
	associated_metadata_check,
	gcopy,
	gmove,
	gsync,
	remove_internal_qc_label,
	change_gg_storage_admin_to_read_write,
	change_sa_storage_admin_to_read_write,
	add_verily_read_access,
)
from markdown_generator import generate_markdown_report

current_time_utc = datetime.now(timezone.utc)
formatted_time = current_time_utc.strftime("%Y-%m-%dT%H-%M-%SZ")

GREEN_CHECKMARK = "✅"
RED_X = "❌"

logging.basicConfig(
	level=logging.INFO,
	format="%(asctime)s - %(levelname)s - %(message)s",
	handlers=[
		logging.FileHandler("promote_staging_data_script.log"),
		logging.StreamHandler()
	]
)

tmp_file = "tmp.txt"

# This will also upload the past data promotion reports and combined MANIFEST.tsv's in archive workflow_metadata folder
def gsync_del(source_path, destination_path, workflow_name, dry_run):
	exclude_regex = rf"(archive|{workflow_name})(/.*)?$"
	command = [
		"gcloud",
		"storage",
		"rsync",
		"--delete-unmatched-destination-objects",
		"-r",
		"-x",
		exclude_regex,
		source_path,
		destination_path
	]
	if dry_run:
		command.insert(4, "--dry-run")
	result = subprocess.run(command, check=True, capture_output=True, text=True)
	logging.info(result.stdout)
	logging.error(result.stderr)


def main(args):
	if args.list:
		list_teams()
		sys.exit(0)

	dry_run = not args.promote
	namespaces = ["uat", "curated"]
	client = storage.Client()

	# Subset buckets/datasets based on workflow_name provided
	WORKFLOW_FILTERS = {
		"pmdbs_sc_rnaseq": ("pmdbs-sn-rnaseq", "pmdbs-sc-rnaseq"),
		"pmdbs_bulk_rnaseq": ("pmdbs-bulk-rnaseq",),
		"spatial_visium": ("spatial-visium",),
		"spatial_geomx": ("spatial-geomx",),
		"mouse_sc_rnaseq": ("mouse-sn-rnaseq", "mouse-sc-rnaseq"),
	}
	patterns = WORKFLOW_FILTERS.get(args.workflow_name)
	dev_buckets_version = {
		bucket: workflow_version
		for bucket, workflow_version in unembargoed_dev_buckets_and_workflow_version_outputs.items()
		if any(pattern in bucket for pattern in patterns)
	}
	logging.info(
		"Processing buckets (workflow=%s):\n%s",
		args.workflow_name,
		"\n".join(dev_buckets_version.keys()) if dev_buckets_version else "(none)"
	)

	for bucket, workflow_version in dev_buckets_version.items():
		logging.info("Processing bucket %s with workflow version %s", bucket, workflow_version)

		file_results = {}
		dataset_id = bucket.replace("gs://asap-dev-", "")
		dataset_id_underscore = dataset_id.replace("-", "_")
		for env in namespaces:
			bucket_name = f"asap-{env}-{dataset_id}"
			bucket = client.get_bucket(bucket_name)

			gs_bucket = f"gs://asap-{env}-{dataset_id}"
			dirs = list_dirs(f"gs://{bucket_name}")
			if args.workflow_name in dirs:
				# Data integrity tests
				logging.info(f"Running data integrity tests on [{bucket_name}]")
				blob_names, gs_files, sample_list_loc = list_gs_files(bucket, args.workflow_name)
				if len(sample_list_loc) > 0:
					previous_curated_outputs_exist = True
					logging.info("Previous curated outputs exist")
					combined_manifest_df = read_manifest_files(bucket, args.workflow_name)
					md5_hashes = md5_check(bucket, args.workflow_name)
					file_results[env] = {
						"blob_names": blob_names,
						"gs_files": gs_files,
						"sample_list_loc": sample_list_loc,
						"combined_manifest_df": combined_manifest_df,
						"md5_hashes": md5_hashes,
					}
				else:
					previous_curated_outputs_exist = False
					logging.info("Previous curated outputs do not exist")
			else:
				previous_curated_outputs_exist = False
				logging.info("Previous curated outputs do not exist")

		bucket = client.get_bucket(f"asap-uat-{dataset_id}")
		not_empty_test_results = non_empty_check(bucket, args.workflow_name, GREEN_CHECKMARK, RED_X)
		metadata_present_test_results = associated_metadata_check(file_results["uat"]["combined_manifest_df"], file_results["uat"]["blob_names"], GREEN_CHECKMARK, RED_X)
		data_integrity_test_results = {**not_empty_test_results, **metadata_present_test_results}
		all_tests_result_status = "True"
		all_tests_result = GREEN_CHECKMARK
		for file_name, result in data_integrity_test_results.items():
			if RED_X in result:
				all_tests_result_status = "False"
				all_tests_result = RED_X
				break

		# Generate report
		generate_markdown_report(
			formatted_time,
			"uat",
			dataset_id,
			dataset_id_underscore,
			args.workflow_name,
			file_results,
			not_empty_test_results,
			metadata_present_test_results,
			all_tests_result_status,
			all_tests_result
		)

		# Try syncing staging data to production
		# --------------------------------------------------------------------------------------------------------
		# DEV and UAT won't always mirror each other.
		# If a team is embargoed, it'll not live in UAT, but for testing purposes, it could live in DEV.
		# Steps:
		# 1. DEV for unembargoed + embargoed teams
		# 2. UAT for unembargoed teams
		# 3. UAT -> PROD
		# Therefore, only promote UAT to PROD.
		# --------------------------------------------------------------------------------------------------------

		if all_tests_result_status == "True":
			raw_bucket = f"gs://asap-raw-{dataset_id}"
			staging_dev_bucket = f"gs://asap-dev-{dataset_id}"
			staging_uat_bucket = f"gs://asap-uat-{dataset_id}"
			production_bucket = f"gs://asap-curated-{dataset_id}"

			production_workflow_bucket = f"gs://asap-curated-{dataset_id}/{args.workflow_name}"
			production_workflow_version_bucket = f"gs://asap-curated-{dataset_id}/{args.workflow_name}/archive/workflow_version/{workflow_version}"
			production_workflow_metadata_path = f"{production_workflow_version_bucket}/workflow_metadata/{formatted_time}"

			dev_workflow_metadata_path = f"{staging_dev_bucket}/{args.workflow_name}/archive/workflow_version/{workflow_version}/workflow_metadata/{formatted_time}"
			uat_workflow_metadata_path = f"{staging_uat_bucket}/{args.workflow_name}/archive/workflow_version/{workflow_version}/workflow_metadata/{formatted_time}"
			file_results["uat"]["combined_manifest_df"].to_csv(f"{dataset_id_underscore}_MANIFEST.tsv", index=False, sep="\t")

			cohort = "cohort" in dataset_id

			if dry_run:
				logging.info(f"Would copy {dataset_id_underscore}_MANIFEST.tsv to {dev_workflow_metadata_path}/MANIFEST.tsv and {uat_workflow_metadata_path}/MANIFEST.tsv")
				logging.info(f"Would copy {dataset_id_underscore}_data_promotion_report.md to {dev_workflow_metadata_path}/data_promotion_report.md and {uat_workflow_metadata_path}/data_promotion_report.md")
				logging.info(f"Would remove internal-qc-data label from [{raw_bucket}]")
				if not cohort:
					logging.info(f"Would grant storage.objectViewer permission to asap-cloud-readers@verily-bvdp.com on [{raw_bucket}]")
					logging.info(f"Would remove storage.admin permission and grant storage.objectViewer and storage.objectCreator permission to CRN Team's SA and GG on [{raw_bucket}]")
			else:
				logging.info(f"Uploading combined manifest and report for [{dataset_id}]")
				gcopy(f"{dataset_id_underscore}_MANIFEST.tsv", f"{dev_workflow_metadata_path}/MANIFEST.tsv")
				gcopy(f"{dataset_id_underscore}_MANIFEST.tsv", f"{uat_workflow_metadata_path}/MANIFEST.tsv")
				gcopy(f"{dataset_id_underscore}_data_promotion_report.md", f"{dev_workflow_metadata_path}/data_promotion_report.md")
				gcopy(f"{dataset_id_underscore}_data_promotion_report.md", f"{uat_workflow_metadata_path}/data_promotion_report.md")
				logging.info(f"Removing internal-qc-data label from [{raw_bucket}]")
				remove_internal_qc_label(raw_bucket)
				if not cohort:
					logging.info(f"Granting storage.objectViewer permission to asap-cloud-readers@verily-bvdp.com on [{raw_bucket}]")
					add_verily_read_access(raw_bucket)
					logging.info(f"Removing Storage Admin access and granting Storage Object Creator and Viewer to CRN Teams for [{raw_bucket}]")
					change_gg_storage_admin_to_read_write(raw_bucket)
					change_sa_storage_admin_to_read_write(raw_bucket)

			logging.info(f"Promoting [{dataset_id}] data to production")
			logging.info(f"\tStaging bucket:\t\t[{staging_uat_bucket}]")
			logging.info(f"\tProduction bucket:\t[{production_bucket}]")
			gsync_del(f"{staging_uat_bucket}/{args.workflow_name}", f"{production_bucket}/{args.workflow_name}", args.workflow_name, dry_run)
			gsync_del(staging_uat_bucket, production_bucket, args.workflow_name, dry_run)

			# Folder path must exist for rsync
			if not previous_curated_outputs_exist:
				with open(tmp_file, "w") as f:
					pass
				gmove(tmp_file, f"{production_workflow_version_bucket}/")
			logging.info(f"Saving workflow version [{workflow_version}] of [{dataset_id}] production data in archive folder")
			logging.info(f"\tProduction bucket path:\t\t[{production_workflow_bucket}]")
			logging.info(f"\tProduction archive/workflow_version bucket path:\t[{production_workflow_version_bucket}]")
			# Dry and real run will only work if staging data gets promoted to production in previous step
			gsync_del(production_workflow_bucket, production_workflow_version_bucket, args.workflow_name, dry_run)

			if dry_run:
				logging.info(f"Would copy {uat_workflow_metadata_path} to {production_workflow_metadata_path}")
			else:
				# Promote combined manifest and data promotion report from staging to production
				gcopy(uat_workflow_metadata_path, production_workflow_metadata_path, recursive=True)
		else:
			logging.error(f"Data cannot be promoted for [{dataset_id}]; exiting")
			sys.exit(1)

		logging.info("Script complete")


if __name__ == "__main__":
	parser = argparse.ArgumentParser(
		description="Promote data in staging UAT buckets to production."
	)

	parser.add_argument(
		"-l",
		"--list",
		action="store_true",
		required=False,
		help="List available teams."
	)
	parser.add_argument(
		"-w",
		"--workflow-name",
		choices=['pmdbs_sc_rnaseq', 'pmdbs_bulk_rnaseq', 'spatial_visium', 'spatial_geomx', 'mouse_sc_rnaseq'],
		type=str,
		required=True,
		help="Workflow name used as a directory in bucket."
	)
	parser.add_argument(
		"-p",
		"--promote",
		action="store_true",
		required=False,
		help="Promote data (default is dry run)."
	)

	args = parser.parse_args()

	main(args)
