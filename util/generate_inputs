#!/usr/bin/env python3

from datetime import datetime
from argparse import ArgumentParser
import pandas as pd
import re
import json
from ast import literal_eval


today = datetime.today()
date = today.strftime("%Y-%m-%d")


def generate_sample_list(projects):
    sample_list_table = []

    for project in projects:
        team_id = project["team_id"]
        dataset_doi_url = project["dataset_doi_url"]
        if args.workflow_name == "spatial_geomx_analysis":
            for slide in project["slides"]:
                for sample in slide["samples"]:
                    sample_id = sample["sample_id"]
                    sample_list_table.append({
                        "team_id": team_id,
                        "sample_id": sample_id,
                        "dataset_doi_url": dataset_doi_url
                    })
        else:
            for sample in project["samples"]:
                sample_id = sample["sample_id"]
                sample_list_table.append({
                    "team_id": team_id,
                    "sample_id": sample_id,
                    "dataset_doi_url": dataset_doi_url
                })
    sample_list_df = pd.DataFrame(sample_list_table)
    sorted_unique_team_ids = sorted(sample_list_df["team_id"].unique().tolist())
    if len(sorted_unique_team_ids) > 1:
        team_id_prefix = "asap-cohort"
    else:
        team_id_prefix = team_id
    len_sample_ids = len(sample_list_df["sample_id"])
    len_unique_sample_ids = len(sorted(sample_list_df["sample_id"].unique().tolist()))
    if len_sample_ids != len_unique_sample_ids:
        raise SystemExit(
            f"Duplicate sample IDs found\nNo. of sample IDs: {len_sample_ids}\nNo. of unique sample IDs: {len_unique_sample_ids}"
        )

    return team_id_prefix, sample_list_df


def main(args):
    run_project_cohort_analysis = args.run_project_cohort_analysis

    projects = dict()
    for project_tsv in args.project_tsvs:
        project_df = pd.read_csv(project_tsv, delimiter="\t")
        # Currently all samples for a team share an embargo status
        project_embargo_status = list(set(project_df.embargoed))
        if len(project_embargo_status) > 1:
            raise SystemExit(
                f"More than one embargo status found for samples in {project_tsv}; don't know how to handle this."
            )
        else:
            staging_env = "dev" if project_embargo_status[0] is True else "uat"
            staging_bucket_types = (
                ["dev"] if project_embargo_status[0] is True else ["dev", "uat"]
            )
        for index, row in project_df.iterrows():
            team_id = row.team_id
            dataset_id = row.ASAP_dataset_id
            sample_id = row.ASAP_sample_id
            batch = str(row.batch)
            fastq_R1s = literal_eval(row.fastq_R1s)
            fastq_R2s = literal_eval(row.fastq_R2s)
            fastq_I1s = literal_eval(row.fastq_I1s)
            fastq_I2s = literal_eval(row.fastq_I2s)

            source = row.source
            team_dataset = row.dataset
            dataset_doi_url = row.dataset_DOI_url

            if args.workflow_name == "spatial_visium_analysis":
                visium_brightfield_image = f"gs://asap-raw-{team_id}-{source}-{team_dataset}/spatial/images/" + row.visium_cytassist
                visium_slide_serial_number = row.visium_slide_ref
                visium_capture_area = row.visium_capture_area

            if args.workflow_name == "spatial_geomx_analysis":
                geomx_slide_id = row.ASAP_geomx_slide_id
                cleaned_geomx_annotation_file = ("cleaned_DNAstack_" + row.geomx_annotation_file.replace(" ", "_")).rsplit(".", 1)[0] + ".xlsx"
                geomx_lab_annotation_xlsx = f"gs://asap-raw-{team_id}-{source}-{team_dataset}/spatial/annotation_files/" + cleaned_geomx_annotation_file

            sample = {
                "sample_id": sample_id,
                "fastq_R1s": fastq_R1s,
                "fastq_R2s": fastq_R2s,
                "fastq_I1s": fastq_I1s,
                "fastq_I2s": fastq_I2s,
            }

            if not pd.isna(batch):
                sample["batch"] = batch

            if args.workflow_name == "spatial_visium_analysis":
                sample["visium_brightfield_image"] = visium_brightfield_image
                sample["visium_slide_serial_number"] = visium_slide_serial_number
                sample["visium_capture_area"] = visium_capture_area

            if args.workflow_name == "spatial_geomx_analysis":
                slide = {
                    "slide_id": geomx_slide_id,
                    "geomx_lab_annotation_xlsx": geomx_lab_annotation_xlsx,
                    "samples": [sample]
                }

            if team_id not in projects:
                staging_data_buckets = [
                    f"gs://asap-{staging_bucket_type}-{team_id}-{source}-{team_dataset}"
                    for staging_bucket_type in staging_bucket_types
                ]

                projects[team_id] = {
                    "team_id": team_id,
                    "dataset_id": dataset_id,
                    "dataset_doi_url": dataset_doi_url,
                    "samples": [sample],
                    "run_project_cohort_analysis": run_project_cohort_analysis,
                    "raw_data_bucket": f"gs://asap-raw-{team_id}-{source}-{team_dataset}",
                    "staging_data_buckets": staging_data_buckets,
                }

                if args.workflow_name == "pmdbs_bulk_rnaseq_analysis":
                    projects[team_id]["project_sample_metadata_csv"] = f"gs://asap-raw-{team_id}-{source}-{team_dataset}/metadata/release/SAMPLE.csv"

                if args.workflow_name == "spatial_geomx_analysis":
                    projects[team_id] = {
                        "team_id": team_id,
                        "dataset_id": dataset_id,
                        "dataset_doi_url": dataset_doi_url,
                        "slides": [slide],
                        "run_project_cohort_analysis": run_project_cohort_analysis,
                        "raw_data_bucket": f"gs://asap-raw-{team_id}-{source}-{team_dataset}",
                        "staging_data_buckets": staging_data_buckets,
                    }
                    projects[team_id]["project_sample_metadata_csv"] = f"gs://asap-raw-{team_id}-{source}-{team_dataset}/metadata/release/SAMPLE.csv"
                    projects[team_id]["geomx_config_ini"] = "File"
            else:
                if args.workflow_name == "spatial_geomx_analysis":
                    slide = next(
                        (s for s in projects[team_id]["slides"] if s["slide_id"] == geomx_slide_id),
                        None
                    )
                    if slide is None:
                        slide = {
                            "slide_id": geomx_slide_id,
                            "geomx_lab_annotation_xlsx": geomx_lab_annotation_xlsx,
                            "samples": [sample]
                        }
                        projects[team_id]["slides"].append(slide)
                    else:
                        slide["samples"].append(sample)
                else:
                    projects[team_id]["samples"].append(sample)

    projects = [project for project in projects.values()]

    with open(args.inputs_template, "r") as f:
        inputs_json = json.load(f)

    projects_key = [
        key for key in inputs_json.keys() if re.search(r"^[^\.]*\.projects$", key)
    ]

    if len(projects_key) != 1:
        raise SystemExit(
            f"Failed to find projects key in the inputs template\nProjects keys found: {projects_key}\nAvailable keys: {inputs_json.keys()}"
        )
    else:
        projects_key = projects_key[0]

        inputs_json[projects_key] = projects

    cohort_raw_data_bucket = f"gs://asap-raw-cohort-{source}-{args.cohort_dataset}"
    inputs_json[
        f"{args.workflow_name}.cohort_raw_data_bucket"
    ] = cohort_raw_data_bucket
    cohort_staging_data_buckets = [
        f"gs://asap-{staging_bucket_type}-cohort-{source}-{args.cohort_dataset}"
        for staging_bucket_type in staging_bucket_types
    ]
    inputs_json[
        f"{args.workflow_name}.cohort_staging_data_buckets"
    ] = cohort_staging_data_buckets

    team_id_prefix, sample_list_df = generate_sample_list(projects)

    inputs_output_filename = f"inputs.{staging_env}.{source}-{args.cohort_dataset}.{date}.json"
    with open(inputs_output_filename, "w") as f:
        json.dump(inputs_json, f, indent=4)
    print(f"Wrote input JSON file: {inputs_output_filename}")

    samples_output_filename = f"{team_id_prefix}.{source}-{args.cohort_dataset}.sample_list.{date}.tsv"
    sample_list_df.to_csv(f"{samples_output_filename}", sep="\t", index=False)
    print(f"Wrote sample list TSV file: {samples_output_filename}")


if __name__ == "__main__":
    parser = ArgumentParser(
        description="Given a TSV of sample information, generate an inputs JSON and sample list"
    )

    parser.add_argument(
        "-p",
        "--project-tsv",
        dest="project_tsvs",
        type=str,
        action="append",
        required=True,
        help="Project TSV including information for samples present in the project; columns team_id, sample_id, batch, fastq_path. Can provide one per project, or include all samples in a single TSV.",
    )
    parser.add_argument(
        "-i",
        "--inputs-template",
        type=str,
        required=True,
        help="Template JSON file to add project information to (projects will be added at the *.projects key).",
    )
    parser.add_argument(
        "-c",
        "--run-project-cohort-analysis",
        action="store_true",
        required=False,
        help="Run project-level cohort analysis. This will be set for all projects included in the cohort [false].",
    )
    parser.add_argument(
        "-w",
        "--workflow-name",
        type=str,
        choices=[
            "pmdbs_sc_rnaseq_analysis",
            "pmdbs_bulk_rnaseq_analysis",
            "spatial_visium_analysis",
            "spatial_geomx_analysis"
        ],
        required=True,
        help="WDL workflow name.",
    )
    parser.add_argument(
        "-b",
        "--cohort-dataset",
        type=str,
        required=True,
        help="Dataset name in cohort bucket name without source (e.g. sc-rnaseq).",
    )

    args = parser.parse_args()

    main(args)
